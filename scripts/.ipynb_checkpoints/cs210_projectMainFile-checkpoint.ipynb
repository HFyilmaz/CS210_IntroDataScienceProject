{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52af4ef",
   "metadata": {
    "id": "a52af4ef"
   },
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b29da3",
   "metadata": {
    "executionInfo": {
     "elapsed": 296,
     "status": "ok",
     "timestamp": 1685135271589,
     "user": {
      "displayName": "Hasan Fırat Yılmaz (Student)",
      "userId": "05379837590085915958"
     },
     "user_tz": -180
    },
    "id": "a3b29da3"
   },
   "outputs": [],
   "source": [
    "# This key belongs to Hasan Firat\n",
    "#API_KEY=\"AIzaSyB5ky0ToQhbvXCA9-4hRh8Kl-jcEX0M3a8\"\n",
    "\n",
    "# Anıl's api:\n",
    "API_KEY=\"AIzaSyD1UOcSN6iwihCm__KcEPp128TPDNsWDNY\"\n",
    "\n",
    "'''\n",
    "channel_ids=[\"UCtTHvHWkbCO2H9Ua8HA8ekA\", #medyali tv\n",
    "                \"UCapDJ1RRsp5cNB-PMdPzVyw\",#Kendine Muhabir\n",
    "                \"UCVSgtPIie4rli7dysK0tjTQ\", #sokaktan al haberi\n",
    "                 \"UCZ5aOEWFOopXLeiIUd2mfJw\"#BabalaTV\n",
    "            ]\n",
    "'''\n",
    "\n",
    "# It is redefined (must redefined) when new channels are added to the list. Do not forget to redefine in the writing csv part too!!\n",
    "channel_ids=[[\"Babala TV\", \"UCZ5aOEWFOopXLeiIUd2mfJw\", 34], \n",
    "            [\"Medyalı TV\", \"UCtTHvHWkbCO2H9Ua8HA8ekA\", 293], \n",
    "            [\"Kendine Muhabir\", \"UCapDJ1RRsp5cNB-PMdPzVyw\", 115], \n",
    "             [\"Sokaktan Al Haberi\", \"UCVSgtPIie4rli7dysK0tjTQ\", 113],\n",
    "             [\"Özlem Gürses\",\"UCojOP7HHZvM2nZz4Rwnd6-Q\",362],\n",
    "             [\"Nevşin Mengü\",\"UCrG27KDq7eW4YoEOYsalU9g\", 145],\n",
    "             [\"Cüneyt Özdemir\", \"UCkwHQ7DWv9aqEtvAOSO74dQ\", 1245],\n",
    "             [\"Mevzular Açık Mikrofon\", \"UCWl9g7avNGKdJPEzsMQOnKw\", 116],\n",
    "             [\"Fatih Portalal TV\", \"UCTRxpG0DLS9eNmeeqTsz_jQ\", 567],\n",
    "             [\"Yeni Şafak\", \"UCClO1RgRkaOcC9cLj-bLuEw\", 231],\n",
    "             [\"SÖZCÜ Televizyonu\", \"UCOulx_rep5O4i9y6AyDqVvw\", 3642]]\n",
    "             \n",
    "youtube=build(\"youtube\",\"v3\",developerKey=API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621ccf99",
   "metadata": {
    "id": "621ccf99"
   },
   "source": [
    "## GET CHANNEL STATS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5213ed",
   "metadata": {
    "id": "2c5213ed"
   },
   "outputs": [],
   "source": [
    "def get_channel_stats(youtube,channel_id):\n",
    "    dataList=[]\n",
    "    \n",
    "    request = youtube.channels().list(\n",
    "        part=\"snippet,contentDetails,statistics\",\n",
    "        id=channel_id\n",
    "    )\n",
    "    response=request.execute()\n",
    "    \n",
    "    for i in range(len(response[\"items\"])):\n",
    "    \n",
    "        data={}\n",
    "        data[\"title\"]=response[\"items\"][i][\"snippet\"][\"title\"]\n",
    "        data[\"description\"]=response[\"items\"][i][\"snippet\"][\"description\"]\n",
    "        data[\"total_view\"]=response[\"items\"][i][\"statistics\"][\"viewCount\"]\n",
    "        data[\"total_video\"]=response[\"items\"][i][\"statistics\"][\"videoCount\"]\n",
    "        data[\"total_subscriber\"]=response[\"items\"][i][\"statistics\"][\"subscriberCount\"]\n",
    "        data[\"playlist_id\"]=response[\"items\"][i][\"contentDetails\"][\"relatedPlaylists\"][\"uploads\"]\n",
    "        dataList.append(data)\n",
    "    \n",
    "    return dataList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cc5a7c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 286,
     "status": "ok",
     "timestamp": 1685125405183,
     "user": {
      "displayName": "Hasan Fırat Yılmaz (Student)",
      "userId": "05379837590085915958"
     },
     "user_tz": -180
    },
    "id": "c2cc5a7c",
    "outputId": "04750520-cc21-4fb4-bc81-d12cdecea18f"
   },
   "outputs": [],
   "source": [
    "channelStats=get_channel_stats(youtube,\"UCZ5aOEWFOopXLeiIUd2mfJw\")\n",
    "print(channelStats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d6f6a0",
   "metadata": {
    "id": "75d6f6a0"
   },
   "outputs": [],
   "source": [
    "channelData=pd.DataFrame(channelStats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0ec5fe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 386,
     "status": "ok",
     "timestamp": 1685125408576,
     "user": {
      "displayName": "Hasan Fırat Yılmaz (Student)",
      "userId": "05379837590085915958"
     },
     "user_tz": -180
    },
    "id": "eb0ec5fe",
    "outputId": "346303be-f345-4137-fbd9-0c70459c11e2"
   },
   "outputs": [],
   "source": [
    "channelData.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc0fd43",
   "metadata": {
    "id": "bfc0fd43"
   },
   "outputs": [],
   "source": [
    "channelData[\"total_view\"]=pd.to_numeric(channelData[\"total_view\"])\n",
    "channelData[\"total_video\"]=pd.to_numeric(channelData[\"total_video\"])\n",
    "channelData[\"total_subscriber\"]=pd.to_numeric(channelData[\"total_subscriber\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019b8e87",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "executionInfo": {
     "elapsed": 278,
     "status": "ok",
     "timestamp": 1685125414148,
     "user": {
      "displayName": "Hasan Fırat Yılmaz (Student)",
      "userId": "05379837590085915958"
     },
     "user_tz": -180
    },
    "id": "019b8e87",
    "outputId": "10451cdd-e858-4236-e31c-07e560e18eee"
   },
   "outputs": [],
   "source": [
    "channelData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca830bf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 412
    },
    "executionInfo": {
     "elapsed": 495,
     "status": "ok",
     "timestamp": 1685125424089,
     "user": {
      "displayName": "Hasan Fırat Yılmaz (Student)",
      "userId": "05379837590085915958"
     },
     "user_tz": -180
    },
    "id": "fca830bf",
    "outputId": "7da1fe72-fe4d-410a-e048-c6376f8bb456"
   },
   "outputs": [],
   "source": [
    "sns.set(rc={\"figure.figsize\":(6,4)})\n",
    "ax=sns.barplot(x=\"title\",y=\"total_view\",data=channelData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ae674f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 399
    },
    "executionInfo": {
     "elapsed": 412,
     "status": "ok",
     "timestamp": 1685125425401,
     "user": {
      "displayName": "Hasan Fırat Yılmaz (Student)",
      "userId": "05379837590085915958"
     },
     "user_tz": -180
    },
    "id": "96ae674f",
    "outputId": "03d05dc6-03f8-4124-c3df-1e07fa8e4f82"
   },
   "outputs": [],
   "source": [
    "ax=sns.barplot(x=\"title\",y=\"total_video\",data=channelData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e72d97e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 412
    },
    "executionInfo": {
     "elapsed": 669,
     "status": "ok",
     "timestamp": 1685125427432,
     "user": {
      "displayName": "Hasan Fırat Yılmaz (Student)",
      "userId": "05379837590085915958"
     },
     "user_tz": -180
    },
    "id": "3e72d97e",
    "outputId": "c1d64034-df39-4d95-95bb-8f0602561330"
   },
   "outputs": [],
   "source": [
    "ax=sns.barplot(x=\"title\",y=\"total_subscriber\",data=channelData)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914b0de3",
   "metadata": {
    "id": "914b0de3"
   },
   "source": [
    "### GETTING ALL VIDEOS FROM A PLAYLIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29984798",
   "metadata": {
    "id": "29984798"
   },
   "outputs": [],
   "source": [
    "# This method gets all the videos from the playlist\n",
    "def get_video_ids(youtube,playlist_id):\n",
    "    \n",
    "    videoList=[]\n",
    "    \n",
    "        \n",
    "    request=youtube.playlistItems().list(part=\"contentDetails\",playlistId=playlist_id,maxResults=50)\n",
    "    \n",
    "    response=request.execute()\n",
    "    \n",
    "    for i in range(len(response[\"items\"])):\n",
    "        video_id=response[\"items\"][i][\"contentDetails\"][\"videoId\"]\n",
    "        videoList.append(video_id)\n",
    "        \n",
    "    next_page_token=response.get(\"nextPageToken\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    while next_page_token is not None:\n",
    "        request=youtube.playlistItems().list(part=\"contentDetails\",playlistId=playlist_id,maxResults=50,pageToken=next_page_token)\n",
    "    \n",
    "        response=request.execute()\n",
    "    \n",
    "        for i in range(len(response[\"items\"])):\n",
    "            video_id=response[\"items\"][i][\"contentDetails\"][\"videoId\"]\n",
    "            videoList.append(video_id)\n",
    "        \n",
    "        next_page_token=response.get(\"nextPageToken\")\n",
    "    \n",
    "    return videoList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3feaa504",
   "metadata": {
    "id": "3feaa504"
   },
   "outputs": [],
   "source": [
    "playlist_id = channelData[\"playlist_id\"][0] #\"UUZ5aOEWFOopXLeiIUd2mfJw\"\n",
    "\n",
    "video_ids=get_video_ids(youtube, playlist_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "o9XosBhic3uB",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 279,
     "status": "ok",
     "timestamp": 1685125449028,
     "user": {
      "displayName": "Hasan Fırat Yılmaz (Student)",
      "userId": "05379837590085915958"
     },
     "user_tz": -180
    },
    "id": "o9XosBhic3uB",
    "outputId": "7b06658c-864c-4a29-9a6b-63c65639b817"
   },
   "outputs": [],
   "source": [
    "print(len(video_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea94605",
   "metadata": {
    "id": "0ea94605"
   },
   "source": [
    "### GETTING VIDEO DETAILS FROM A VIDEO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a574f219",
   "metadata": {
    "id": "a574f219"
   },
   "outputs": [],
   "source": [
    "# Gets the statics of the each video of the channels, can be any number of video.\n",
    "def get_video_details(youtube,video_ids):\n",
    "    \n",
    "    \n",
    "    video_stats_list=[] #will include dictionaries\n",
    "    \n",
    "    for i in range(0,len(video_ids),50):\n",
    "    \n",
    "        request=youtube.videos().list(part=\"snippet,statistics\",id=\",\".join(video_ids[i:i+50]))\n",
    "\n",
    "        response=request.execute()\n",
    "        \n",
    "        for i in range(len(response[\"items\"])):\n",
    "            video_stats={}\n",
    "            video_stats[\"video_title\"]=response[\"items\"][i][\"snippet\"][\"title\"]\n",
    "            #video_stats[\"video_description\"]=response[\"items\"][i][\"snippet\"][\"description\"]\n",
    "            video_stats[\"publish_date\"]=response[\"items\"][i][\"snippet\"][\"publishedAt\"]\n",
    "            video_stats[\"view_count\"]=response[\"items\"][i][\"statistics\"][\"viewCount\"]\n",
    "            #video_stats[\"like_count\"]=response[\"items\"][i][\"statistics\"][\"likeCount\"]\n",
    "            #video_stats[\"comment_count\"]=response[\"items\"][i][\"statistics\"][\"commentCount\"]\n",
    "            \n",
    "            video_stats_list.append(video_stats)\n",
    "            \n",
    "    return video_stats_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0933f86",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 269,
     "status": "ok",
     "timestamp": 1685125461970,
     "user": {
      "displayName": "Hasan Fırat Yılmaz (Student)",
      "userId": "05379837590085915958"
     },
     "user_tz": -180
    },
    "id": "d0933f86",
    "outputId": "600ed246-5a07-42d4-960a-d88ddb982745"
   },
   "outputs": [],
   "source": [
    "detail_list = get_video_details(youtube,video_ids[:10])\n",
    "print(\"All the list:\\n\", detail_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Xlp3iYPeeaAA",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1685125464034,
     "user": {
      "displayName": "Hasan Fırat Yılmaz (Student)",
      "userId": "05379837590085915958"
     },
     "user_tz": -180
    },
    "id": "Xlp3iYPeeaAA",
    "outputId": "48bc0c06-6d20-44dd-a626-934e7e5126a9"
   },
   "outputs": [],
   "source": [
    "print(\"Length of the list: \", len(detail_list))\n",
    "print(\"First video of the list: \", detail_list[0])\n",
    "print(\"Video id of the first video: \", video_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8a6f57",
   "metadata": {
    "id": "af8a6f57"
   },
   "outputs": [],
   "source": [
    "dfBabalaTV=pd.DataFrame(detail_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f15314",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 283,
     "status": "ok",
     "timestamp": 1685125468308,
     "user": {
      "displayName": "Hasan Fırat Yılmaz (Student)",
      "userId": "05379837590085915958"
     },
     "user_tz": -180
    },
    "id": "84f15314",
    "outputId": "8f616a0b-f3bd-4cc9-bf69-c1f63bfff46a"
   },
   "outputs": [],
   "source": [
    "dfBabalaTV.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a575d6a",
   "metadata": {
    "id": "3a575d6a"
   },
   "outputs": [],
   "source": [
    "dfBabalaTV[\"view_count\"]=pd.to_numeric(dfBabalaTV[\"view_count\"])\n",
    "#dfKendine[\"like_count\"]=pd.to_numeric(dfKendine[\"like_count\"])\n",
    "#dfKendine[\"comment_count\"]=pd.to_numeric(dfKendine[\"comment_count\"])\n",
    "dfBabalaTV['publish_date'] = pd.to_datetime(dfBabalaTV['publish_date'],  format='%Y-%m-%dT%H:%M:%SZ').dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b25e197",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1685125471998,
     "user": {
      "displayName": "Hasan Fırat Yılmaz (Student)",
      "userId": "05379837590085915958"
     },
     "user_tz": -180
    },
    "id": "6b25e197",
    "outputId": "97d54467-c7b4-469f-d6dd-a6d473cd4728"
   },
   "outputs": [],
   "source": [
    "dfBabalaTV.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cd044c",
   "metadata": {
    "id": "f4cd044c"
   },
   "outputs": [],
   "source": [
    "newest_oldest=dfBabalaTV.sort_values(by=\"publish_date\",ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a5580f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 362
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1685125478864,
     "user": {
      "displayName": "Hasan Fırat Yılmaz (Student)",
      "userId": "05379837590085915958"
     },
     "user_tz": -180
    },
    "id": "27a5580f",
    "outputId": "5830df5d-d5ac-40f1-c89e-b6bc02cc75c1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "newest_oldest.tail(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cddd6bb",
   "metadata": {
    "id": "3cddd6bb"
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7955776",
   "metadata": {
    "id": "b7955776"
   },
   "outputs": [],
   "source": [
    "contains_secim = dfBabalaTV[\"video_title\"].str.contains('Cumhurbaşkanı', flags=re.IGNORECASE, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad9319c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 379,
     "status": "ok",
     "timestamp": 1685125485021,
     "user": {
      "displayName": "Hasan Fırat Yılmaz (Student)",
      "userId": "05379837590085915958"
     },
     "user_tz": -180
    },
    "id": "5ad9319c",
    "outputId": "dad6bc71-66f3-4850-de18-5c18742a01ba"
   },
   "outputs": [],
   "source": [
    "contains_secim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4720ad73",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 601
    },
    "executionInfo": {
     "elapsed": 359,
     "status": "ok",
     "timestamp": 1685125488413,
     "user": {
      "displayName": "Hasan Fırat Yılmaz (Student)",
      "userId": "05379837590085915958"
     },
     "user_tz": -180
    },
    "id": "4720ad73",
    "outputId": "44001d08-31c4-4635-b7f0-6adc09d36948"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot the line graph\n",
    "plt.plot(dfBabalaTV[\"publish_date\"], dfBabalaTV[\"view_count\"])\n",
    "\n",
    "# Set x-axis label\n",
    "plt.xlabel(\"Publish Date\")\n",
    "\n",
    "# Set y-axis label\n",
    "plt.ylabel(\"View Count\")\n",
    "\n",
    "# Set the x-axis ticks with 10-day intervals and rotate the labels by 45 degrees\n",
    "plt.xticks(pd.date_range(start=dfBabalaTV[\"publish_date\"].min(), end=dfBabalaTV[\"publish_date\"].max(), freq=\"20D\"), rotation=90)\n",
    "\n",
    "# Adjust the layout to prevent x-axis label cutoff\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tuOTpvqdTscA",
   "metadata": {
    "id": "tuOTpvqdTscA"
   },
   "outputs": [],
   "source": [
    "# Returns last 100 comments --> needs update, we don't want to return last comments, we want to return random 100 comments\n",
    "def get_100_comments(youtube,video_id):\n",
    "    \n",
    "    commentList={}\n",
    "    \n",
    "        \n",
    "    request=youtube.commentThreads().list(part=\"snippet\",videoId=video_id,maxResults=100)\n",
    "    \n",
    "    response=request.execute()\n",
    "\n",
    "    \n",
    "    for i in range(len(response[\"items\"])):\n",
    "        comment=response[\"items\"][i][\"snippet\"][\"topLevelComment\"][\"snippet\"][\"textDisplay\"]\n",
    "        publishDate=response[\"items\"][i][\"snippet\"][\"topLevelComment\"][\"snippet\"][\"publishedAt\"]\n",
    "        commentList[comment] = publishDate\n",
    "        \n",
    "    return commentList\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wJdjXXEFab1Z",
   "metadata": {
    "id": "wJdjXXEFab1Z"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def get_100_random_comments(youtube, video_id):\n",
    "    commentList = {}\n",
    "    nextPageToken = None\n",
    "    \n",
    "    while len(commentList) < 100:\n",
    "        request = youtube.commentThreads().list(\n",
    "            part=\"snippet\",\n",
    "            videoId=video_id,\n",
    "            maxResults=100,\n",
    "            pageToken=nextPageToken\n",
    "        )\n",
    "        response = request.execute()\n",
    "\n",
    "        for item in response[\"items\"]:\n",
    "            comment = item[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"textDisplay\"]\n",
    "            publishDate = item[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"publishedAt\"]\n",
    "            commentList[comment] = publishDate\n",
    "\n",
    "            if len(commentList) >= 100:\n",
    "                break\n",
    "\n",
    "        nextPageToken = response.get(\"nextPageToken\")\n",
    "\n",
    "        if not nextPageToken or len(commentList) >= 100:\n",
    "            break\n",
    "\n",
    "    if len(commentList) < 100:\n",
    "        print(\"There are fewer than 100 comments available.\")\n",
    "    \n",
    "    random_comments = random.sample(list(commentList.items()), min(100, len(commentList)))\n",
    "    random_comment_dict = dict(random_comments)\n",
    "    \n",
    "    return random_comment_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440UxKhOYP5x",
   "metadata": {
    "id": "440UxKhOYP5x"
   },
   "outputs": [],
   "source": [
    "commentList100_random = get_100_random_comments(youtube, \"HT9sbK7uJTs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "C1ixMhh8d7_B",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 283,
     "status": "ok",
     "timestamp": 1685128237526,
     "user": {
      "displayName": "Hasan Fırat Yılmaz (Student)",
      "userId": "05379837590085915958"
     },
     "user_tz": -180
    },
    "id": "C1ixMhh8d7_B",
    "outputId": "5628063d-7996-4452-abaf-a468a17933a2"
   },
   "outputs": [],
   "source": [
    "print(detail_list[7])\n",
    "print(video_ids[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CzUOEylfaivD",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 340,
     "status": "ok",
     "timestamp": 1685128169253,
     "user": {
      "displayName": "Hasan Fırat Yılmaz (Student)",
      "userId": "05379837590085915958"
     },
     "user_tz": -180
    },
    "id": "CzUOEylfaivD",
    "outputId": "7d12f990-2e33-4186-9289-0841b23412eb"
   },
   "outputs": [],
   "source": [
    "print(len(commentList100_random))\n",
    "commentList100_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6LaB_T4eTu6a",
   "metadata": {
    "id": "6LaB_T4eTu6a"
   },
   "outputs": [],
   "source": [
    "# Returns a list which contains all the comments\n",
    "def get_all_comments(youtube,video_id):\n",
    "    \n",
    "    commentList={}\n",
    "    \n",
    "        \n",
    "    request=youtube.commentThreads().list(part=\"snippet\",videoId=video_id,maxResults=100)\n",
    "    \n",
    "    response=request.execute()\n",
    "    \n",
    "    for i in range(len(response[\"items\"])):\n",
    "        comment=response[\"items\"][i][\"snippet\"][\"topLevelComment\"][\"snippet\"][\"textDisplay\"]\n",
    "        publishDate=response[\"items\"][i][\"snippet\"][\"topLevelComment\"][\"snippet\"][\"publishedAt\"]\n",
    "        commentList[comment] = publishDate\n",
    "        \n",
    "        \n",
    "    next_page_token=response.get(\"nextPageToken\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    while next_page_token is not None:\n",
    "        request=youtube.commentThreads().list(part=\"snippet\",videoId=video_id,maxResults=100,pageToken=next_page_token)\n",
    "    \n",
    "        response=request.execute()\n",
    "    \n",
    "        for i in range(len(response[\"items\"])):\n",
    "            comment=response[\"items\"][i][\"snippet\"][\"topLevelComment\"][\"snippet\"][\"textDisplay\"]\n",
    "            publishDate=response[\"items\"][i][\"snippet\"][\"topLevelComment\"][\"snippet\"][\"publishedAt\"]\n",
    "            \n",
    "            commentList[comment] = publishDate\n",
    "        \n",
    "        next_page_token=response.get(\"nextPageToken\")\n",
    "    \n",
    "    return commentList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_edUWl16UO1j",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 304,
     "status": "ok",
     "timestamp": 1685125759433,
     "user": {
      "displayName": "Hasan Fırat Yılmaz (Student)",
      "userId": "05379837590085915958"
     },
     "user_tz": -180
    },
    "id": "_edUWl16UO1j",
    "outputId": "8943dddc-c00c-4e1f-a792-daf07dec1c14"
   },
   "outputs": [],
   "source": [
    "commentList_100 = get_100_comments(youtube, video_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1GxGK0UwU_77",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1685125940625,
     "user": {
      "displayName": "Hasan Fırat Yılmaz (Student)",
      "userId": "05379837590085915958"
     },
     "user_tz": -180
    },
    "id": "1GxGK0UwU_77",
    "outputId": "0f62d519-7d63-4a6d-a769-cbd574e95eb8"
   },
   "outputs": [],
   "source": [
    "print(commentList_100)\n",
    "print(detail_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e64678c",
   "metadata": {
    "id": "5e64678c"
   },
   "outputs": [],
   "source": [
    "commentListAll = get_all_comments(youtube,\"HT9sbK7uJTs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e1e9ae",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 355,
     "status": "ok",
     "timestamp": 1685127135202,
     "user": {
      "displayName": "Hasan Fırat Yılmaz (Student)",
      "userId": "05379837590085915958"
     },
     "user_tz": -180
    },
    "id": "02e1e9ae",
    "outputId": "3b7a31a5-6d42-4300-eb66-c94883ca7254"
   },
   "outputs": [],
   "source": [
    "print(commentListAll)\n",
    "print(len(commentListAll))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174799f4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1685126202528,
     "user": {
      "displayName": "Hasan Fırat Yılmaz (Student)",
      "userId": "05379837590085915958"
     },
     "user_tz": -180
    },
    "id": "174799f4",
    "outputId": "d4442ed9-ae34-4671-a915-e478d5a6cde0"
   },
   "outputs": [],
   "source": [
    "commentList_100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "YA2m_NCUWr-u",
   "metadata": {
    "id": "YA2m_NCUWr-u"
   },
   "source": [
    "# **Complex Function**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KwceXwsBfpv5",
   "metadata": {
    "id": "KwceXwsBfpv5"
   },
   "outputs": [],
   "source": [
    "def get_video_detail(youtube,video_id):\n",
    "   \n",
    "    request=youtube.videos().list(part=\"snippet,statistics\",id=video_id)\n",
    "\n",
    "    response=request.execute()\n",
    "    \n",
    "    \n",
    "    video_stats={}\n",
    "    video_stats[\"video_title\"]=response[\"items\"][0][\"snippet\"][\"title\"]\n",
    "    video_stats[\"video_description\"]=response[\"items\"][0][\"snippet\"][\"description\"]\n",
    "    video_stats[\"publish_date\"]=response[\"items\"][0][\"snippet\"][\"publishedAt\"]\n",
    "    video_stats[\"view_count\"]=response[\"items\"][0][\"statistics\"][\"viewCount\"]\n",
    "    video_stats[\"like_count\"]=response[\"items\"][0][\"statistics\"][\"likeCount\"]\n",
    "    video_stats[\"comment_count\"]=response[\"items\"][0][\"statistics\"][\"commentCount\"]\n",
    "            \n",
    "    return video_stats    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "EZQCX4HNfbjC",
   "metadata": {
    "id": "EZQCX4HNfbjC"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def get_february_details(youtube, video_ids):\n",
    "    video_ids_list = []  # will include video IDs\n",
    "    today = datetime.today().date()\n",
    "    cutoff_date = datetime(2023, 2, 1).date()\n",
    "\n",
    "    for i in range(0, len(video_ids), 50):\n",
    "        request = youtube.videos().list(part=\"snippet,statistics\", id=\",\".join(video_ids[i:i + 50]))\n",
    "        response = request.execute()\n",
    "\n",
    "        for item in response[\"items\"]:\n",
    "            video_id = item[\"id\"]\n",
    "            publish_date_str = item[\"snippet\"][\"publishedAt\"]\n",
    "            publish_date = datetime.strptime(publish_date_str, \"%Y-%m-%dT%H:%M:%SZ\").date()\n",
    "            if cutoff_date <= publish_date <= today:\n",
    "                video_ids_list.append(video_id)\n",
    "\n",
    "    return video_ids_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "K9LMyDmXWlpE",
   "metadata": {
    "id": "K9LMyDmXWlpE"
   },
   "outputs": [],
   "source": [
    "import html\n",
    "import re\n",
    "\n",
    "# Creates a dictionary for specified channels (handles the exception of commentDisabled)\n",
    "def createDictForChannel(youtube, channel_id):\n",
    "    channel_info_dict = get_channel_stats(youtube, channel_id)\n",
    "    channelName = channel_info_dict[0][\"title\"].replace(\"\\\"\", \"\\'\")\n",
    "    channelPlaylist = channel_info_dict[0][\"playlist_id\"]\n",
    "    video_ids = get_video_ids(youtube, channelPlaylist)\n",
    "    video_ids = get_february_details(youtube, video_ids)\n",
    "    \n",
    "    MAIN_DICT = {channelName: {}}\n",
    "    \n",
    "    for video_id in video_ids:\n",
    "        try:\n",
    "            videoStatDict = get_video_detail(youtube, video_id)\n",
    "            videoTitle = videoStatDict[\"video_title\"].replace(\"\\\"\", \"\\'\")\n",
    "            commentDict = get_100_random_comments(youtube, video_id)\n",
    "            \n",
    "            if videoTitle not in MAIN_DICT[channelName]:\n",
    "                MAIN_DICT[channelName][videoTitle] = []\n",
    "            \n",
    "            # Append each comment and its publish date to MAIN_DICT[channelName][videoTitle]\n",
    "            for comment, publish_date in commentDict.items():\n",
    "                comment = html.unescape(comment)\n",
    "                comment = re.sub(r'<.*?>', '', comment)\n",
    "                comment = comment.replace(\"\\\"\", \"\\'\")\n",
    "                MAIN_DICT[channelName][videoTitle].append({comment: publish_date})\n",
    "        \n",
    "        except Exception as e:\n",
    "            if \"commentsDisabled\" in str(e):\n",
    "                print(\"Comments are disabled for the video:\", video_id)\n",
    "            else:\n",
    "                print(\"An error occurred for video\", video_id + \":\", str(e))\n",
    "    \n",
    "    return MAIN_DICT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sS33jK7ptTx0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1906,
     "status": "ok",
     "timestamp": 1685132447898,
     "user": {
      "displayName": "Hasan Fırat Yılmaz (Student)",
      "userId": "05379837590085915958"
     },
     "user_tz": -180
    },
    "id": "sS33jK7ptTx0",
    "outputId": "18324263-b1c4-4e5b-c7a3-4fb9e1e5ed35"
   },
   "outputs": [],
   "source": [
    "kendinePlaylist = get_channel_stats(youtube, \"UCVSgtPIie4rli7dysK0tjTQ\")[0][\"playlist_id\"]\n",
    "videoIdsKendine = get_video_ids(youtube, kendinePlaylist)\n",
    "videoIdsKendine = get_february_details(youtube, video_ids)\n",
    "\n",
    "print(\"Number of videos that are released by Kendine Muhabir since february: \", len(videoIdsKendine))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Atm7l6_sWoiC",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20661,
     "status": "ok",
     "timestamp": 1685131307351,
     "user": {
      "displayName": "Hasan Fırat Yılmaz (Student)",
      "userId": "05379837590085915958"
     },
     "user_tz": -180
    },
    "id": "Atm7l6_sWoiC",
    "outputId": "25acf8dc-7626-4a73-83ba-24229bab3034"
   },
   "outputs": [],
   "source": [
    "# Testing purposes!\n",
    "kendineMuhabirDict=createDictForChannel(youtube,\"UCVSgtPIie4rli7dysK0tjTQ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6S4WdIkVW3hI",
   "metadata": {
    "id": "6S4WdIkVW3hI"
   },
   "source": [
    "# **WRITING TO A CSV FILE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22J3s4eiWqFK",
   "metadata": {
    "id": "22J3s4eiWqFK"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def write_dict_to_csv(main_dict, filename):\n",
    "    # Open the CSV file in write mode\n",
    "    with open(filename, \"w\", newline=\"\",encoding=\"utf-8\") as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        \n",
    "        # Write the header row\n",
    "        writer.writerow([\"channelName\", \"video_title\", \"sentence\", \"publishDate\"])\n",
    "        \n",
    "        # Write the data rows\n",
    "        for channel_name, videos in main_dict.items():\n",
    "            for video_title, comments in videos.items():\n",
    "                for comment_data in comments:\n",
    "                    for sentence, publish_date in comment_data.items():\n",
    "                        writer.writerow([channel_name, video_title, sentence, publish_date])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3baowHByW-UQ",
   "metadata": {
    "id": "3baowHByW-UQ"
   },
   "outputs": [],
   "source": [
    "# Testing purposes!\n",
    "write_dict_to_csv(kendineMuhabirDict,\"kendineMuhabir.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dNHYaelW_34",
   "metadata": {
    "id": "4dNHYaelW_34"
   },
   "outputs": [],
   "source": [
    "# Testing purposes!\n",
    "df=pd.read_csv(\"kendineMuhabir.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DBdL7UxqW_xe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "executionInfo": {
     "elapsed": 286,
     "status": "ok",
     "timestamp": 1685131346515,
     "user": {
      "displayName": "Hasan Fırat Yılmaz (Student)",
      "userId": "05379837590085915958"
     },
     "user_tz": -180
    },
    "id": "DBdL7UxqW_xe",
    "outputId": "12fb69d5-a4b2-49af-f3ab-74f6d057b56c"
   },
   "outputs": [],
   "source": [
    "# Testing purposes!\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Yuae6CxDXE9H",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 263,
     "status": "ok",
     "timestamp": 1685131743772,
     "user": {
      "displayName": "Hasan Fırat Yılmaz (Student)",
      "userId": "05379837590085915958"
     },
     "user_tz": -180
    },
    "id": "Yuae6CxDXE9H",
    "outputId": "0642aff0-3de0-4031-fd1c-2204530759af"
   },
   "outputs": [],
   "source": [
    "# Testing purposes!\n",
    "distinct_values = df[\"video_title\"].nunique()\n",
    "print(\"Number of distinct values:\", distinct_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yM0HZefGsSZN",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 721538,
     "status": "error",
     "timestamp": 1685133433147,
     "user": {
      "displayName": "Hasan Fırat Yılmaz (Student)",
      "userId": "05379837590085915958"
     },
     "user_tz": -180
    },
    "id": "yM0HZefGsSZN",
    "outputId": "f63df758-6d5d-47ba-e4f7-c384281d4b43"
   },
   "outputs": [],
   "source": [
    "# Getting all comments for different documents using channel_ids list.\n",
    "# Works well, commentDisabled error is handled successfully after the fix!\n",
    "channel_ids=[[\"BabalaTV\", \"UCZ5aOEWFOopXLeiIUd2mfJw\", 34], \n",
    "            [\"MedyalıTV\", \"UCtTHvHWkbCO2H9Ua8HA8ekA\", 293], \n",
    "            [\"KendineMuhabir\", \"UCapDJ1RRsp5cNB-PMdPzVyw\", 115], \n",
    "             [\"SokaktanAlHaberi\", \"UCVSgtPIie4rli7dysK0tjTQ\", 113],\n",
    "             [\"ÖzlemGürses\",\"UCojOP7HHZvM2nZz4Rwnd6-Q\",362],\n",
    "             [\"NevşinMengü\",\"UCrG27KDq7eW4YoEOYsalU9g\", 145],\n",
    "             [\"CüneytÖzdemir\", \"UCkwHQ7DWv9aqEtvAOSO74dQ\", 1245],\n",
    "             [\"MevzularAçıkMikrofon\", \"UCWl9g7avNGKdJPEzsMQOnKw\", 116],\n",
    "             [\"FatihPortakalTV\", \"UCTRxpG0DLS9eNmeeqTsz_jQ\", 567],\n",
    "             [\"YeniŞafak\", \"UCClO1RgRkaOcC9cLj-bLuEw\", 231],\n",
    "             [\"SÖZCÜTelevizyonu\", \"UCOulx_rep5O4i9y6AyDqVvw\", 3642]]\n",
    "\n",
    "for each in channel_ids:\n",
    "  print(\"Creating dictionary and writing into a csv file for: \", each[0])\n",
    "  creatingDict = createDictForChannel(youtube, each[1])\n",
    "  write_dict_to_csv(creatingDict, each[0] + \".csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbbe2eb",
   "metadata": {},
   "source": [
    "# Chapter 2 - Text Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16c4a7f",
   "metadata": {},
   "source": [
    "### In this chapter, comments that have been gathered are going to put in some processes and by using a model we are going to get their sentiment score and which president candidate they are related with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f680492f",
   "metadata": {},
   "source": [
    "### Google translator is used since we could not find any Turkish sentiment analyzer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c47a55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import html\n",
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "\n",
    "\n",
    "def translate_to_english(text):\n",
    "    try:\n",
    "        returnTxt = GoogleTranslator(source='tr', target='en').translate(html.unescape(text))\n",
    "        if returnTxt is None:\n",
    "            return \" \"\n",
    "        else:\n",
    "            return returnTxt\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred during translation:\", str(e))\n",
    "        return \" \"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e544b15",
   "metadata": {},
   "source": [
    "### In order to deal with emojis, emoji dictionary obtained online is used and emojis are replaced with textual representations. Such as ❤️ -> \"red heart\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b9aeaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "\n",
    "with open('scripts/Emoji_Dict.p', 'rb') as fp:\n",
    "    Emoji_Dict = pickle.load(fp)\n",
    "    Emoji_Dict = {v: k for k, v in Emoji_Dict.items()}\n",
    "\n",
    "def convert_emojis_to_word(text):\n",
    "    for emot in Emoji_Dict:\n",
    "        converted_word = \"[\" + \"_\".join(Emoji_Dict[emot].replace(\",\", \"\").replace(\":\", \"\").split()) + \"]\"\n",
    "        pattern = re.escape(emot)\n",
    "        text = re.sub(pattern, converted_word, text)\n",
    "    \n",
    "    text=text.replace(\"[\",\" \").replace(\"]\",\" \").replace(\"_\",\" \")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4d1e20",
   "metadata": {},
   "source": [
    "#### Lemmatization technique is also used to make sentiment analyzer model's job easier. It takes list of words as input and returns lemmatized list of words. Order of the occurences of the words in actual sentence are the same as the returned list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "102482c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\anil17\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\anil17\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def lemmatize(list_of_words):\n",
    "\n",
    "    py_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Tag the words with POS\n",
    "    pos_tags = pos_tag(list_of_words)\n",
    "\n",
    "    py_lemword = []\n",
    "    for word, tag in pos_tags:\n",
    "        # Map the POS tag to WordNet POS tags\n",
    "        wn_tag = wordnet.NOUN\n",
    "        if tag.startswith('V'):\n",
    "            wn_tag = wordnet.VERB\n",
    "        elif tag.startswith('J'):\n",
    "            wn_tag = wordnet.ADJ\n",
    "        elif tag.startswith('R'):\n",
    "            wn_tag = wordnet.ADV\n",
    "\n",
    "        # Lemmatize the word with the appropriate POS tag\n",
    "        lem_word = py_lemmatizer.lemmatize(word, pos=wn_tag)\n",
    "        py_lemword.append(lem_word)\n",
    "        \n",
    "    return py_lemword"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88186d36",
   "metadata": {},
   "source": [
    "### To identify the president candidates in 2023 election, no pre-defined models are used and the words associated with each candidate are determined manually.                                                                                                                                                                 [Note that even if Muharrem İnce announced his withdrawal from the race, because it happened only several days before the election, we still believed the importance of him being put into our data analysis since most of the period of the presidential race, he took part as candidate.}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9c64ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "KKList= [\"kemal\",\"kılıçdar\",\"gılıçdar\",\"kemalkkılıçdaroğlu\",\n",
    "         \"kiliçdar\",\"kilicdar\",\"kılıçtar\",\"kilicdarogllu\",\"piro\", \" kk \"]\n",
    "\n",
    "RTEList=[\"rte\",\"reis\",\"erdogan\",\"tayyip\",\"recep\",\"receb\",\"erduvan\",\"erdoğan\",\"tayyib\",\"erdogaan\",\"erdogann\",\"tayyyip\"]\n",
    "\n",
    "SOList=[\"sinan\",\"ogan\",\"s.ogan\",\"s.oğan\"\"sinanogan\",\"oğan\",\"ssogan\"\n",
    "    \"sinanogann\",\n",
    "    \"sinanogğan\",\n",
    "    \"sinanoğgan\",\n",
    "    \"sinanoğaann\",\n",
    "    \"sinanogaan\",\n",
    "    \"sinanoğgaan\",\n",
    "    \"sinanogaann\",\n",
    "    \"sinanogğaan\",\n",
    "    \"sinanoğaan\",]\n",
    "\n",
    "MIList= [\n",
    "    \"muharrem ince\",\n",
    "    \"muharem ince\",\n",
    "    \"muharrem inçe\",\n",
    "    \"muharem inçe\",\n",
    "    \"muharrem i̇nce\",\n",
    "    \"muharem i̇nce\",\n",
    "    \"muharrem i̇nçe\",\n",
    "    \"muharem i̇nçe\",\n",
    "    \"muharremi̇nce\",\n",
    "    \"muharemince\",\n",
    "    \"muharreminçe\",\n",
    "    \"muharemi̇nçe\",\n",
    "    \"muharremince\",\n",
    "    \"muharremi̇nçe\",\n",
    "    \"marrem\",\"muharrem\",\"mince\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81170419",
   "metadata": {},
   "source": [
    "### Below function takes comment as parameter and gives 1 score for each occurences of the words in politician's associated words list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d8ad7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def whichPolitician(sentence, KKList, RTEList, SOList, MIList):\n",
    "    try:\n",
    "        sentence_lower = sentence.lower()\n",
    "        KKscore = sum([1 for word in KKList if word in sentence_lower])\n",
    "        RTEscore = sum([1 for word in RTEList if word in sentence_lower])\n",
    "        SOscore = sum([1 for word in SOList if word in sentence_lower])\n",
    "        MIscore = sum([1 for word in MIList if word in sentence_lower])\n",
    "        \n",
    "        return KKscore, RTEscore, SOscore, MIscore\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred during processing:\", str(e))\n",
    "        return 0, 0, 0, 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f57d0c",
   "metadata": {},
   "source": [
    "### Online sentiment model is used to identify the sentiment score for English sentences. It gives a score between 1-5 which represents negative sentiment and positive sentiment respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "825a3248",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "sentiment_classifier = pipeline(\"sentiment-analysis\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "\n",
    "# Analyze the sentiment of a sentence\n",
    "#sentence = 'i believed once, he said that the nation alliance will win, it was a lie, i will not believe it again'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62b40310",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiment(sentence):\n",
    "    \n",
    "    result = sentiment_classifier(sentence)[0]\n",
    "    \n",
    "    return result[\"label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ed9649",
   "metadata": {},
   "source": [
    "### Below function is using the above functions and outputting the sentiment score using the model. This function is helpful to achieve integrity while processing the comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a4feae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import html\n",
    "import re\n",
    "def textSentiment(text):\n",
    "    #for sentiment analysis , 512 max is suppported\n",
    "    text=text[:512]\n",
    "  \n",
    "    #translate text\n",
    "    text_translated=translate_to_english(text)\n",
    "    \n",
    "    #handling emojis\n",
    "    text_translated=convert_emojis_to_word(text)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #text is from now on a list of words\n",
    "    \n",
    "    text_list = re.findall(r'\\b\\w+\\b', text_translated)\n",
    "    text_list=lemmatize(text_list)\n",
    "    \n",
    "    \n",
    "    text_manipulated=\" \".join(text_list)\n",
    "    \n",
    "    text_manipulated=text_manipulated[:512]\n",
    "    #now text is a sentence again\n",
    "    #ready for sentiment analysis without stopwords,handled emojis,lemmatized\n",
    "\n",
    "    text_sentiment_score=analyze_sentiment(text_manipulated)\n",
    "    text_sentiment_score=int(text_sentiment_score[0])\n",
    "    \n",
    "    return text_sentiment_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbc9bb6",
   "metadata": {},
   "source": [
    "# Chapter 3 - Applying sentiment and candidate scores to csv files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa33efa",
   "metadata": {},
   "source": [
    "### Initially, dividing into our dataframe into chunks was not in our plans, but due to sentiment model taking more time than expected, we ended up using chunks technique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1032b68",
   "metadata": {},
   "source": [
    "### Initially, for each channel we randomly pick 2000 comments and creating our 2K chunks for each channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7e914f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "df=pd.read_csv(\"data_sources/raw_dataset/CüneytÖzdemir.csv\")\n",
    "\n",
    "num_rows = 2000\n",
    "randomly_selected_rows = df.sample(n=num_rows, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "0762be89",
   "metadata": {},
   "outputs": [],
   "source": [
    "randomly_selected_rows.to_csv('data_sources/reduced_dataset_2K/CüneytÖzdemir2K.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e200e62c",
   "metadata": {},
   "source": [
    "### Except for Ahsen TV and Babala TV, all 2K chunks of every channel are without sentiment right now (We used them to estimate how long our model takes). We have 12 channels besides that two channel. We aim to seperate 24k comments into 5 chunks and put their sentiment score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "e4ed5f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV files merged and divided into chunks successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "from math import ceil\n",
    "\n",
    "# Define the path to the folder containing the CSV files\n",
    "folder_path = 'data_sources/reduced_dataset_2K'\n",
    "\n",
    "# Get a list of all CSV files in the folder\n",
    "csv_files = glob.glob(os.path.join(folder_path, '*.csv'))\n",
    "\n",
    "# Read and concatenate all CSV files into a single DataFrame\n",
    "data = pd.concat((pd.read_csv(file) for file in csv_files))\n",
    "\n",
    "# Calculate the number of rows per chunk\n",
    "num_chunks = 5\n",
    "rows_per_chunk = ceil(len(data) / num_chunks)\n",
    "\n",
    "# Create the 'chunks' folder if it doesn't exist\n",
    "chunks_folder = 'chunks'\n",
    "if not os.path.exists(chunks_folder):\n",
    "    os.makedirs(chunks_folder)\n",
    "\n",
    "# Split the data into chunks and save them as separate CSV files\n",
    "for i in range(num_chunks):\n",
    "    start_index = i * rows_per_chunk\n",
    "    end_index = (i + 1) * rows_per_chunk\n",
    "    chunk_data = data[start_index:end_index]\n",
    "    chunk_file_path = os.path.join(chunks_folder, f'chunk{i + 1}.csv')\n",
    "    chunk_data.to_csv(chunk_file_path, index=False)\n",
    "\n",
    "print(\"CSV files merged and divided into chunks successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3545932",
   "metadata": {},
   "source": [
    "### Reading chunk by chunk and get their sentiment score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c9799c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv(\"data_sources/chunked_data/before_sentiment_analysed_chunks/chunk1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a5cf0493",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"sentiment_score\"]=df[\"sentiment_score\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c2465e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████████████████████████████████████████████████████▋                        | 3342/4800 [41:27<23:37,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred during translation: 0 --> text must be a valid text with maximum 5000 character,otherwise it cannot be translated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 4800/4800 [59:59<00:00,  1.33it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for index, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    sentence = row[\"sentence\"]\n",
    "    sentiment_score = textSentiment(sentence)\n",
    "    df.at[index, \"sentiment_score\"] = sentiment_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "edeb87df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"data_sources/chunked_data/after_sentiment_analysed_chunks/chunk1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc41eb1b",
   "metadata": {},
   "source": [
    "### Merging 5 chunks containing 4800 rows into 1 main dataset. In the end, we have 24000 comments with sentiment scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8066d1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Directory containing the CSV files\n",
    "directory = \"../data_sources/chunked_data/after_sentiment_analysed_chunks\"\n",
    "\n",
    "# Initialize an empty list to store the DataFrames\n",
    "data_frames = []\n",
    "\n",
    "# Iterate over the CSV files in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        # Read each CSV file into a DataFrame\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Append the DataFrame to the list\n",
    "        data_frames.append(df)\n",
    "\n",
    "# Concatenate the DataFrames in the list\n",
    "df = pd.concat(data_frames, ignore_index=True)\n",
    "\n",
    "# Write the merged data to a new CSV file\n",
    "df.to_csv(\"../data_sources/uncleaned_dataset/MAINDATA.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af47807b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
